{
  "agent_id": "coder3",
  "task_id": "task_2",
  "files": [
    {
      "filename": "model_hypothesis_layer.py",
      "purpose": "ModelHypothesis neurons implementing gradient descent with fixed-point arithmetic",
      "priority": "high",
      "dependencies": [
        "lava-nc",
        "numpy"
      ],
      "key_functions": [
        "create_model_hypothesis_neurons",
        "implement_fixed_point_update",
        "configure_gradient_descent"
      ],
      "estimated_lines": 400,
      "complexity": "high"
    }
  ],
  "project_info": {
    "project_name": "NeuroRF_Neuromorphic_Robust_Fitting",
    "project_type": "computer_vision",
    "description": "A neuromorphic implementation of robust geometric model fitting using spiking neural networks on Intel Loihi 2 hardware. The project implements event-driven formulations for minimal subset sampling, model estimation via gradient descent, and model verification to achieve 85% energy savings compared to CPU-based RANSAC while maintaining equivalent accuracy for linear regression and affine image registration tasks.",
    "key_algorithms": [
      "Spiking Neural Network Gradient Descent",
      "Event-driven Random Sampling",
      "Integer Arithmetic Approximation",
      "Fixed-point Gradient Updates",
      "Convolution-based Matrix Multiplication Emulation"
    ],
    "main_libraries": [
      "lava-nc",
      "numpy",
      "opencv-python",
      "scipy",
      "matplotlib",
      "pandas",
      "pyJoules",
      "vlfeat",
      "transformers"
    ]
  },
  "paper_content": "PDF: cs.NE_2508.09466v1_Event-driven-Robust-Fitting-on-Neuromorphic-Hardwa.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nEvent-driven Robust Fitting on Neuromorphic Hardware\nTam Ngoc-Bang Nguyen1, Anh-Dzung Doan1, Zhipeng Cai2, Tat-Jun Chin1\n1Australian Institute for Machine Learning, The University of Adelaide,\n2Intel Labs\n{tam.nb.nguyen,dzung.doan,tat-jun.chin }@adelaide.edu.au\nczptc2h@gmail.com\nAbstract\nRobust fitting of geometric models is a fundamental task\nin many computer vision pipelines. Numerous innovations\nhave been produced on the topic, from improving the effi-\nciency and accuracy of random sampling heuristics to gen-\nerating novel theoretical insights that underpin new ap-\nproaches with mathematical guarantees. However, one as-\npect of robust fitting that has received little attention is en-\nergy efficiency. This performance metric has become criti-\ncal as high energy consumption is a growing concern for AI\nadoption. In this paper, we explore energy-efficient robust\nfitting via the neuromorphic computing paradigm. Specifi-\ncally, we designed a novel spiking neural network for robust\nfitting on real neuromorphic hardware, the Intel Loihi 2.\nEnabling this are novel event-driven formulations of model\nestimation that allow robust fitting to be implemented in the\nunique architecture of Loihi 2, and algorithmic strategies\nto alleviate the current limited precision and instruction set\nof the hardware. Results show that our neuromorphic ro-\nbust fitting consumes only a fraction (15%) of the energy\nrequired to run the established robust fitting algorithm on a\nstandard CPU to equivalent accuracy.\n1. Introduction\nMany computer vision pipelines, ranging from visual\nSLAM, 3D reconstruction to image stitching, need\nto estimate geometric models from noisy and outlier-\ncontaminated measurements that occur when operating in\nreal-world environments [48]. Often, this is achieved by\noptimising the model parameters according to a robust cri-\nterion defined over the data, known as robust fitting .\nRandom sampling heuristics such as RANSAC [29] and\nits variants [46] are established techniques for robust fit-\nting. Such methods usually deliver satisfactory outcomes,\nbut provide little insights on the veracity of the results [64].\nOn the other hand, techniques that rely on mathematical\nprogramming can provide some quality guarantees, but aretypically too slow to be practical on real data [17].\nSignificant research has been devoted into robust fitting\nfor computer vision. This includes improving the speed,\naccuracy and generalisability of random sampling meth-\nods [8, 21, 57] and deriving theoretical insights to bet-\nter inform the design and usage of mathematical program-\nming techniques ( e.g., adapting to special cases, relaxing\nthe guarantees) [42, 55]. Machine learning approaches that\ncan leverage statistics in training data to hypothesis sam-\npling have also been developed [11]. Recently, quantum\ncomputing has also been explored for robust fitting [18].\nAn aspect of robust fitting algorithm research that has\nreceived comparatively little attention is energy efficiency .\nWith the rapidly rising energy consumption of AI systems\nbecoming a concern [22], it is vital to develop vision al-\ngorithms that are energy-efficient. We argue that devising\nlow-power alternatives for core components such as robust\nfitting is an essential step to enable ambitious low-power\nend-to-end 3D vision pipelines.\nIn this paper, we explore energy-efficient robust fitting\nvia neuromorphic computing, which is a bio-inspired com-\nputational model where a network of processing units called\nspiking neurons asynchronously send spike-based messages\nto each other [60]. Such a structure is called a spiking neu-\nral network (SNN). Due to the massive parallelism, stochas-\ntic behaviour, and event-driven computing, SNNs promise\nhigher energy efficiency than conventional computing, in-\ncluding artificial neural networks (ANN) [35].\nCurrently available neuromorphic processors that can\nimplement SNNs include IBM TrueNorth [49], SpiN-\nNaker [34] and Intel Loihi 1 and 2 [24, 53, 62]. Comprehen-\nsive experiments [25] indicate the much higher energy effi-\nciency of the neuromorphic devices, which underlines their\npotential in reducing the energy consumption of data cen-\ntres [3] and their application on embodied AI systems [44].\nContributions We develop an SNN that can conduct ro-\nbust fitting on neuromorphic hardware, specifically Intel\nLoihi 2 [62]. Underpinning our SNN are novel event-driven\nformulations of core steps in robust fitting, namely minimalarXiv:2508.09466v1  [cs.CV]  13 Aug 2025\n\n--- Page 2 ---\nsubset sampling, model estimation and model verification,\nthat make the problem amenable to a neuromorphic treat-\nment. We also propose strategies to mitigate the current\nlimitations on precision and instruction set of Loihi 2.\nWhen simulated on the CPU, results of our SNN on\nsynthetic data and real datasets verify its correctness and\ncompetitive accuracy relative to state-of-the-art robust fit-\nting methods. Importantly, experiments on Loihi 2 illustrate\nthe vastly superior energy efficiency of our SNN, in that it\nconsumes only a fraction (15%) of the energy required by\nestablished methods on the CPU.\n2. Related work\n2.1. Computing paradigms for robust fitting\nDue to the crucial role of robust fitting in computer vision,\nvarious computing paradigms and hardware platforms have\nbeen explored to accelerate their execution.\nFPGAs offer highly parallel computing capabilities,\nmaking them well-suited for real-time robust fitting tasks.\nSeveral works have explored robust fitting on FPGAs, lever-\naging the mechanism of parallel hypothesis evaluation to\nimprove performance [67\u201369]. However, challenges such\nas limited on-chip memory, as well as difficulties in pro-\ngramming, optimising, and debugging algorithms, make\nFPGA implementations nontrivial [61].\nBy devising differentiable versions of robust fitting and\nreformulating the problem as a machine learning task [12,\n13, 70], GPUs have been adopted to conduct robust fitting.\nHowever, inferencing large neural networks on GPUs can\nbe energy intensive [5, 31, 32]. Also, machine learning ap-\nproaches suffer from generalizability issues if the testing\ndata distribution differs from the training data distribution.\nRecently, robust fitting using quantum computing has\ngained significant interest within the research commu-\nnity [17, 26, 72]. These studies have shown promising po-\ntential for accelerating robust fitting processes. However,\nthe quantum approaches remain in an experimental phase,\nwith testing limited to small-scale data due to current hard-\nware limitations. Furthermore, quantum computers have\nhigh energy demands due to cooling requirements [45, 51].\nUnlike the above, we show that neuromorphic comput-\ning offers both high energy efficiency and usability.\n2.2. Neuromorphic computing for optimisation\nResearch on algorithms in neuromorphic computing can\nbe broadly categorized into two main directions: machine\nlearning and optimization [25, 60]. Learning approaches\nfocus on converting pre-trained ANNs into corresponding\nSNNs for on-chip inference, directly learning SNN param-\neters through training equivalent proxy ANNs, and approx-\nimation of backpropagation on neuromorphic hardware. In\nthe optimization domain, the spike-based temporal process-ing characteristics of SNNs are exploited to develop solu-\ntions for optimization tasks. Our work belongs to the latter.\nThe temporal dynamics of SNNs have been actively ex-\nplored in combinatorial and continuous optimization. In in-\nteger domains, a majority of handcrafted SNNs were de-\nsigned for constraint satisfaction problems (CSPs), includ-\ning travelling salesman problem, Sudoku, Boolean satisfi-\nability and graph coloring [10, 30, 40, 54, 71]. The core\nidea behind these approaches is to encode CSP variables\nand constraints into an SNN topology. The SNN-based CSP\nsolvers iteratively explore and refine the solution space, un-\ntil seeking an assignment for variables that satisfy all con-\nstraints of the original CSP problem. Another line of fo-\ncus in SNN-based combinatorial solvers is quadratic uncon-\nstrained binary optimization (QUBO) [6, 23, 28, 50, 56].\nWhile the primary objective of CSP solvers is to find fea-\nsible assignments to a combinatorial optimization problem,\nQUBO aims to find optimal solutions, where the inherent\nspike-based temporal dynamics of SNNs can be viewed as\niterative solvers for optimization problems [44, 56, 63].\n2.3. Neuromorphic computing for vision\nNeuromorphic computing is receiving increased attention\nin the vision community [15, 59, 66]. [15, 59] designed\nSNN-based for optical flow estimation from event stream\ndata. [66] integrated Loihi chip as a co-processor for com-\nputing angular error for drone control tasks. Neuromor-\nphic computing has been explored in visual place recog-\nnition [37] and SLAM [41]. It is worthwhile to note that\nnot all the works have tested on actual neuromorphic pro-\ncessors. Moreover, few have paid attention to robust fitting.\n3. Preliminaries\nWe first review the problem formulation for robust fitting\nbefore giving a brief overview of SNNs and Intel Loihi 2.\n3.1. Robust fitting\nWe describe the procedure for robustly fitting a linear re-\ngression model, which is the specific problem targeted by\nour proposed SNN. This does not reduce the applicabil-\nity of our ideas since many geometric models can be lin-\nearized [36, Chap. 4]. Moreover, our main aim is to es-\ntablish the viability of neuromorphic robust fitting, and its\nextension to nonlinear models is left as future work.\nGiven a set of Nmeasurements D={(xi, yi)}N\ni=1,\nwhere xi\u2208Rdandyi\u2208R, we wish to estimate the lin-\near equation y=xT\u03b8. The least squares (LS) solution is\n\u02c6\u03b8= arg min\n\u03b8\u2208RdNX\ni=1(yi\u2212xT\ni\u03b8)2= arg min\n\u03b8\u2208Rd\u2225X\u03b8\u2212y\u22252\n2(1)\nwhere X\u2208RN\u00d7dandy\u2208RNare obtained by vertically\nstacking {xT\ni}N\ni=1and{yi}N\ni=1, respectively. If Dcontains\n\n--- Page 3 ---\noutliers, \u02c6\u03b8will be biased. Instead, robust fitting aims to find\nthe model \u03b8that minimizes the objective function\nNX\ni=1\u03c1\u0000\f\fyi\u2212xT\ni\u03b8\f\f\u0001\n, (2)\nwhere \u03c1is a robust loss [74]. Widely used in vision is\n\u03c1(r) =(\n1r > \u03f5 inlier,\n0otherwise,(3)\nwhere \u03f5inlier is the inlier threshold, hence, minimizing (2)\nwith (3) is equivalent to maximizing the inlier count\n\u03a8(\u03b8) =NX\ni=1I\u0000\f\fyi\u2212xT\ni\u03b8\f\f\u2264\u03f5inlier\u0001\n(4)\nof\u03b8, where I(\u00b7)is an indicator function that returns 1 if the\ninput condition is true and 0 otherwise. The value \u03a8(\u03b8)is\ncalled the consensus of \u03b8[16], and the resultant estimate is\nrobust to outliers provided \u03f5inlierwas selected appropriately.\nGenerating model hypotheses \u03b8by sampling minimal\nsubsets is a successful approach for robust fitting [47]. Ba-\nsically, three main steps are repetitively executed:\n1. Sample a d-subset S \u2282 D (minimal subset) of the data.\n2. Estimate a model hypothesis by LS fitting (1) on S.\n3. Evaluate the quality of the hypothesis using (2).\nAt termination, the algorithm returns the model with the\nbest objective value. If the number of repetitions Kis large\nenough, at least one all-inlier minimal subset Swill be sam-\npled, leading to a robust estimate of the model.\n3.2. SNN\nA neuromorphic algorithm can be designed as an SNN that\nis then executed on a neuromorphic computer [60]. Con-\nceptually, an SNN consists of a set of Nspiking neurons,\nwhere each pair of neurons is connected via synapses. The\nsynaptic connection strengths are represented by a weight\nmatrix W= [wij]\u2208RN\u00d7N, where wij= 0 indicates the\nabsence of a connection between the i-th and j-th neurons.\nEach spiking neuron comprises internal states that accu-\nmulate input stimuli over time, and emits spikes only when\npredefined conditions are met. Upon spiking, it transmits\na spike signal to connected neurons and may enter into a\nrefractory (inactive) period. Well-known spiking neuron\nmodels include Leaky Integrate-and-Fire [33], Resonate-\nand-Fire [38] and Izhikevich [39], though the SNN frame-\nwork is flexible enough to allow custom neurons with spe-\ncific computations. The synaptic weights and neuronal pro-\ncessing define the problem that is solved by the SNN.\n3.3. Intel Loihi 2\nEach Loihi 2 chip [53] houses 128 asynchronous neuromor-\nphic cores (neuro cores) which can simulate up to 8192stateful and parallel spiking neurons. At each neuro core,\nthe ingress spikes from other cores enter a Synapse block,\nwhere a dense/ sparse matrix-vector multiplication or con-\nvolution is performed; see Fig. 1. The result is then ac-\ncumulated and passed as inputs to the Neuron block. Here,\nstateful neuron programs are executed and 24-bit spike mes-\nsages are generated and routed to other neuro cores [24, 44].\nThe SNN architecture can be designed using Dense ,Sparse ,\nandConvolution synaptic configurations of 8-bit weights,\nand the neuron models can be customised through assem-\nbly code with up to 24-bit internal states.\n=11111=transposeflattenflattenelement-wisemultiplicationOutSynapseNeuronDendriteaccumulatorAxonIn\nFigure 1. The top diagram shows a high-level schematic of a neuro\ncore in Loihi 2. Note that Neuron block is programmable. The bot-\ntom diagram shows our technique to emulate matrix-matrix multi-\nplication and how it is mapped onto the neuro core. See Fig. AA\nSupplementary for more details.\n4. Neuromorphic robust fitting\nHere, we describe the proposed SNN for neuromorphic\nrobust fitting, including the mathematical underpinnings,\nspiking neuron designs, and hardware implementation de-\ntails. Note that while spiking neurons are conceptually\nasynchronous or event-driven [60], the operations of neu-\nrons below are described using discrete timesteps tto give\nmore intuitive depiction of time evolution. This matches the\nprogramming at the neuron level in real neuromorphic com-\nputers such as Loihi 2, which are fully digital devices [24].\nHowever, it should be reminded that tis the algorithmic\ntime rather than a global synchronous clock.\n4.1. SNN for robust fitting\nFig. 2 illustrates the proposed SNN for robust fitting, called\nNeuroRF , which consists of the following neurons:\n\u2022NRandomSampling neurons {zi}N\ni=1.\n\u2022dModelHypothesis neurons {\u03b8j}d\nj=1.\n\u2022NdAuxiliary neurons {\u03b8\u2032\ni,j}j=1,...,d\ni=1,...,N .\n\u2022NComputeResidual neurons {cj}N\nj=1.\n\u20221InlierCounter neuron \u03a3.\nAlgorithm 1 defines the internal arithmetic operations\nthat evolve the state of each neuron. Note that, unlike in\n\n--- Page 4 ---\nthe case of classical (von Neumann) computers, there is\nno clear main body and parent-subfunction relationships,\nthough the neurons influence each other by sending output\nspikes through the interconnections.\nSampling Model verification Model fitting\nRandomSampling neuronsAuxiliary neurons\nModelHypothesis neurons\nComputeResidual neuronsInlierCounter neuron\nFigure 2. The proposed NeuroRF SNN.\nThe behaviour of NeuroRF will be described in more de-\ntail below via its main operations, while details of imple-\nmentation on Loihi 2 will be provided in Sec. 4.2.\n4.1.1. Random sampling\nAs the name suggests, the role of the RandomSampling neu-\nrons is conducting random sampling of the input data. Each\nziis a binary variable, where zi= 1 means that the i-th\npoint (xi, yi)is selected and zi= 0means otherwise.\nFor a d-dimensional model, each neuron emits a spike\nwith probability d/N . This is encoded in a simple dynamic\nby comparing a randomly generated number with a constant\nd/N . However, since the neurons sample independently,\nthey may not collectively select exactly dpoints. Since we\nconduct estimation using gradient descent (details to fol-\nlow), over- or under-sampling the points (including select-\ning no points) do not cause numerical issues.\n4.1.2. Model hypothesis generation\nThe LS objective (1) can be written in the quadratic form\nf(\u03b8) =\u03b8TQ\u03b8+pT\u03b8, (5)\nwhere Q=XTXandpT=\u2212yTX. Since f(\u03b8)is con-\nvex, it justifies using gradient descent (GD) to solve least\nsquares, where the first-order gradient is\n\u2207f(\u03b8) =Q\u03b8+p, (6)\nand we iteratively update \u03b8via\n\u03b8(t)=\u03b8(t\u22121)\u2212\u03b1\u0010\nQ\u03b8(t\u22121)+p\u0011\n(7)\nwith\u03b1being the step size or learning rate for Miterations.\nPrevious studies have shown that the spike-based tempo-\nral dynamics of SNNs align with the behaviour of classicaliterative solvers [44, 52, 56, 63]. In fact, the GD iteration (7)\nreadily lends itself as the dynamic equations for the Model-\nHypothesis neurons, where each evolves according to\n\u03b8(t)\nj=\u03b8(t\u22121)\nj\u2212\u03b1\u0010\nqj\u03b8(t\u22121)+pj\u0011\n, (8)\nwithqjbeing the j-th row of Qandpjbeing the j-th el-\nement of p. Conceptually, qjandpjare respectively the\nsynaptic weights and bias leading into the j-th neuron.\nHowever, a direct application of (8) is problematic for\nour aims, since the data for estimation is encoded in Qand\np, which need to be repetitively sampled at each robust fit-\nting iteration, while the synaptic weights and biases are con-\nstant in the Synapse block of Loihi 2 during operation.\nWe propose an algebraic manipulation that enables fully\nneuromorphic random sampling and model hypothesis gen-\neration.\nCollecting the RandomSampling states in a vector z=\n[z1, z2, . . . , z N]T, we \u201clift\u201d the gradient (6) as\n\u2207f(\u03b8,z) =Q\u2032\u03b8\u2032+P\u2032z (9)\nwhere\nQ\u2032=\u0002\nx1xT\n1x2xT\n2. . .xNxT\nN\u0003\n\u2208Rd\u00d7Nd,\n\u03b8\u2032= vec\u0000\nz\u03b8T\u0001\n=z\u2297\u03b8\u2208RNd,\nP\u2032= [\u2212y1x1\u2212y2x2. . .\u2212yNxN]\u2208Rd\u00d7N.\n(10)\nNote that the \u2297is the Kronecker product. The lifting allows\nthe gradient to be a function of the data selection via z. The\nGD update is now also dependent on z,i.e.,\n\u03b8(t)=\u03b8(t\u22121)\u2212\u03b1\u0010\nQ\u2032\u03b8\u2032(t\u22121)+P\u2032z\u0011\n. (11)\nSee Sec. A Supplementary for details of the derivation.\nNote that the extreme cases ( z=1andz=0) do not cause\nnumerical issues, since the gradient reduces to \u2207f(\u03b8)and\n0respectively (no operation in the latter case).\nThe expansion (10) creates NdAuxiliary neurons \u03b8\u2032,\nwhich are unrolled into {\u03b8\u2032\ni,j}j=1,...,d\ni=1,...,N . Each Auxiliary neu-\nron evolves according to\n\u03b8\u2032(t)\ni,j=z(t\u22121)\ni\u03b8(t\u22121)\nj, (12)\nwhich behaves as a coupling of a pair of RandomSampling\nand ModelHypothesis neurons. More importantly, the Mod-\nelHypothesis neurons now follow the dynamical equation\n\u03b8(t)\nj=\u03b8(t\u22121)\nj\u2212\u03b1\u0010\nq\u2032\nj\u03b8\u2032(t\u22121)+p\u2032\njz\u0011\n, (13)\nwhere q\u2032\njis the j-th row of Q\u2032andp\u2032\njis the j-th row of\nP\u2032. The synaptics weights and biases leading into the j-th\nModelHypothesis Neuron \u03b8jare now constant .\n\n--- Page 5 ---\nAlgorithm 1 NeuroRF algorithm\nRandomSampling layer\nRequire: Switching probability prob =d\nN\nRequire: \u03c4= 2M+ 4,L=K\u2217\u03c4\n1:Initialize k\u21900, counter \u21900\n2:Initialize z(0)\u21900N\n3:fort= 1,2, . . . ,Ldo\n4: counter += 1\n5: next sampling idx=\u03c4\u2217k+ 1\n6: ifcounter =next sampling idxthen\n7: k+= 1\n8: \u03b3(t)= rand[0 ,1)\n9: z(t)=prob\u2265\u03b3(t)\n10: else\n11: z(t)=z(t\u22121)\n12: Send z(t)to connected Auxiliary and ModelHy-\npothesis layers.\nAuxiliary layer\nRequire: Connection matrices Fd,FN\nRequire: \u03c4= 2M+ 4,L=K\u2217\u03c4\n1:Initialize \u03b8\u2032(0)\u21900Nd\n2:Initialize k\u21901, counter \u21900\n3:fort= 1,2, . . . ,Ldo\n4: zin(t)=Fd\u229bz(t\u22121)\n5: \u03b8in(t)=FN\u229b\u03b8(t\u22121)\n6: counter += 1\n7: next sampling idx=\u03c4\u2217k+ 1\n8: ifcounter =next sampling idxthen\n9: k+= 1\n10: \u03b8\u2032(t)=0Nd\n11: else\n12: \u03b8\u2032(t)=zin(t)\u2217\u03b8in(t)\n13: Send \u03b8\u2032(t)to connected ModelHypothesis layer.\n4.1.3. Model verification\nThe ComputeResidual and InlierCounter neurons calculate\nthe residuals and consensus (4) of the current model esti-\nmate (states of the ModelHypothesis neurons). These are\nstraightforward, and the reader is referred to Algorithm 1.\n4.2. Mitigating hardware limitations\nTo implement NeuroRF on Loihi 2 [62], the SNN architec-\nture and synaptic weights are defined on a host CPU, then\nmapped to the neuro cores of Loihi 2 via the Lava frame-\nwork [1]. Consensus sets found are read out and returned\nto the host CPU once the algorithm finishes. This probe is\nperformed by special embedded processors that can access\nthe states of neuro cores [15, 27].\nHowever, Loihi 2 has several constraints: 8-bit weightsAlgorithm 1 NeuroRF algorithm (cont.)\nModelHypothesis layer\nRequire: Connection matrices Q\u2032,P\u2032and learning rate \u03b1\nRequire: \u03c4= 2M+ 4,L=K\u2217\u03c4\n1:Initialize \u03b8(0)\u21900d\n2:Initialize k\u21901, counter \u21900\n3:fort= 1,2, . . . ,Ldo\n4: ain(t)=Q\u2032\u03b8\u2032(t\u22121)\n5: bias(t)=P\u2032z(t\u22121)\n6: counter += 1\n7: next sampling idx=\u03c4\u2217k+ 1\n8: ifcounter =newsampling idxthen\n9: k+= 1\n10: \u03b8(t)=0d\n11: else\n12: ifcounter is odd then\n13: total grad(t)= ain(t)+ bias(t)\n14: \u03b8(t)=\u03b8(t\u22121)\u2212\u03b1\u2217total grad(t)\n15: else\n16: \u03b8(t)=\u03b8(t\u22121)\n17: Send \u03b8(t)to Auxiliary layer\nComputeResidual layer\nRequire: Connection matrix X, bias vector yand inlier\nthreshold \u03f5inlier\nRequire: \u03c4= 2M+ 4,L=K\u2217\u03c4\n1:Initialize c\u21900N\n2:fort= 1,2, . . . ,Ldo\n3: ain(t)=X\u03b8(t\u22121)\n4: res(t)=ain(t)\u2212y\n5: c(t)=|res(t)| \u2264\u03f5inlier\n6: Sendc(t)to InlierCounter neuron\nInlierCounter neuron\nRequire: Connection matrix 1T\nN\nRequire: \u03c4= 2M+ 4,L=K\u2217\u03c4\n1:fort= 1,2, . . . ,Ldo\n2: \u03a8(t)=1T\nNc(t\u22121)\nwhich can limit the instance size for testing, no support for\nfloating-point arithmetic, a reduced instruction set (no di-\nvision) and synapse configurations with limited customis-\nability [2, 44, 56]. Here, we present strategies to mitigate\nthese constraints. Note that the limitations may be solved in\nfuture iterations of Loihi, and do not detract from the math-\nematical and logical consistencies of NeuroRF.\nInteger arithmetic All data Dis assumed integer or con-\nverted to integer from rational numbers. The optimal model\nestimate \u03b8may not be integral, but NeuroRF is aimed at\n\n--- Page 6 ---\nfinding the best integer solution. As we will show in Sec. 5,\nthe integer approximation does not significantly reduce the\nquality (consensus size) obtained, as observed in other ap-\nplications on Loihi 2 [44, 63].\nRandom sampling Loihi 2\u2019s pseudo-random generator\n(PRG) is restricted to either 16 or 24 bit integers [56]. We\ndescribe our approach to using 16-bit PRG, but the idea is\nsimilar for the 24-bit PRG. Let LFSR \u2208[0,216\u22121]be the\nLoihi 2-generated random number. The required threshold\n\u03b3\u2208[0,1)is mathematically obtained as\n\u03b3=LFSR\n216\u22121\u2248LFSR\n216. (14)\nFrom Algorithm 1, the switching condition for a Random-\nSampling neuron is mathematically defined as\nd >N\u2217LFSR\n216. (15)\nWe approximate the division using the arithmetic right shift,\nleading to the switching logic\nd >((N\u2217LFSR )\u226b16). (16)\nGD update step size The GD process (11) requires a step\nsize parameter \u03b1, which should be sufficiently small to not\ncause divergence. To allow a floating-point \u03b1, we use fixed-\npoint representation [44, 63] for storing \u03b1. Specifically, an\n\u03b1\u2208Rcan be converted to a fixed-point counterpart\n\u00af\u03b1= ceil( \u03b1\u22172\u03b2), (17)\nwhere \u03b2\u2208Zis a tuned constant. Here \u00af\u03b1is rounded up to\nthe nearest integer. The update (11) can be approximated as\n\u03b8(t)=\u03b8(t\u22121)\u2212\u0010\nQ\u2032\u03b8\u2032(t\u22121)+P\u2032z\u0011\u00af\u03b1\n2\u03b2. (18)\nUsing the arithmetic right shift, the GD update becomes\n\u03b8(t)=\u03b8(t\u22121)\u2212h\u0010\nQ\u2032\u03b8\u2032(t\u22121)+P\u2032z\u0011\n\u2217\u00af\u03b1i\n\u226b\u03b2 (19)\nThis approximation at each update step could cause overall\ndrift of the solution [44, 63], thus the number of steps should\nbe kept small to avoid large build-up of errors.\nMatrix-matrix multiplications On Loihi 2, neurons\nmust connect through Synapse (See Fig 1), where an inter-\naction \u03b8\u2032= vec( z\u03b8T)is not feasible. To realise the Auxil-\niary neural dynamics, we introduce two synaptic connection\nmatrices into \u03b8\u2032\n\u03b8\u2032= vech\u0000\n1dzT\u0001Ti\n\u2217vec\u0000\n1N\u03b8T\u0001\n. (20)Here1dand1Ncan be viewed as synaptic weights of\nmatrix-matrix multiplication Synapses, which are not sup-\nported on Loihi 2. To overcome this limitation, we emulate\nmatrix-matrix multiplication with Conv Synapse\n\u0000\n1dzT\u0001\n=Fd\u229bz,\u0000\n1N\u03b8T\u0001\n=FN\u229b\u03b8, (21)\nwhere\u229bis a convolution operation. FN\u2208RN\u00d71\u00d71and\nFd\u2208Rd\u00d71\u00d71are respectively the set of Nandd(1\u00d71)\nconvolutional filters. See Fig. 1 for a visual explanation of\nour approach to emulate matrix-matrix multiplication and\nhow this process is mapped onto neuro cores.\n5. Results\nOur experiments focused on establishing the correctness of\nNeuroRF, its performance on a neuromorphic chip, and its\npotential in lowering the energy cost of robust fitting.\n5.1. Method variants\nTwo different variants of NeuroRF were used:\n\u2022NeuroRF-CPU : NeuroRF was implemented in Lava [1]\nwith 64-bit floating-point arithmetic and x86-64 instruc-\ntion set, and run on an Intel Core i7-11700K. This simu-\nlated NeuroRF on a CPU.\n\u2022NeuroRF-Loihi : NeuroRF was implemented on Loihi 2\n(specifically on the single-chip Oheo Gulch board [2])\nusing Lava-Loihi 0.7.0 [1] and Loihi assembly lan-\nguage [53]. Note that Loihi 2 had limited precision and\ninstruction set, which we mitigated in Sec. 4.2.\n5.2. Correctness of NeuroRF\nWe first validate NeuroRF on robust linear regression via\nsynthetic data. For each data instance, we randomly gen-\nerated a true model \u03b8\u2217andNindependent measurements\n{xi}N\ni=1. Each yiwas then computed based on linear model\ny=\u03b8Txand perturbed with Gaussian noise of \u03c3inlier=\n0.1. To simulate outliers, \u03b7%data points were randomly\ncorrupted with Gaussian noise of \u03c3outlier = 1.5. The inlier\nthreshold \u03f5inlierwas set to 0.5.\nTo rigorously test our method, we generated 65 prob-\nlem instances of varying levels of difficulty, with 5 instances\nproduced for each combination of (N, d, \u03b7 ):\n\u2022N\u2208[100,200,400,300,500] withd= 8 and outlier\nratio\u03b7= 20 .\n\u2022d\u2208[2,3,6,8]withN= 200 and\u03b7= 20 .\n\u2022\u03b7\u2208[10,20,30,40,50,60]withN= 200 andd= 8.\nWe compared NeuroRF and a classical robust fitting al-\ngorithm RANSAC [29], denoted as RS-CPU , running on\nan Intel Core i7-11700K. The number of iterations Kwas\nset to 300for both methods, while M= 200 and\u03b1= 0.02\nwere configured for NeuroRF-CPU. Since the goal here is\nto verify the correctness of our SNN on synthetic data, it is\nsufficient to compare against RS-CPU. We will benchmark\nagainst more advanced methods on real data in Sec. 5.4.\n\n--- Page 7 ---\nTo evaluate robust fitting accuracy, we employed the nor-\nmalized Euclidean distance between the ground-truth and\noutput models, \u03b8gtand\u03b8est, from a robust fitting method:\n100\u2217 \u2225\u03b8gt\u2212\u03b8est\u22252/\u2225\u03b8gt\u22252. (22)\nWe recorded the average and std. dev. of the normalized\ndistance over 10 trials for each method.\nFig. 3 plots the normalized distance across the N,d\nand\u03b7variations. NeuroRF-CPU achieved comparable per-\nformance to RS-CPU on the three setups, which confirms\nthe algorithmic soundness of NeuroRF and its effectiveness\nin handling high-dimensional problems. Also, see Sec. C\nSupp. for runtime results. Note that the runtime figures\nhere were from simulating NeuroRF on a CPU, and hence\nare not reflective of runtime on Loihi 2.\n5.3. Performance on neuromorphic hardware\nDue to the relatively low capacity of Loihi 2, we\ntested NeuroRF-Loihi on line fitting problems only ( d= 2).\nSince the synaptic weights are 8 bits only, we generated\nsynthetic data as follows: in each instance, the true model\n\u03b8\u2217was sampled from {\u221210,\u22129, . . . , 9,10}2, from which\nN\u2208 {10,20}integer points that satisfy the linear relation\nwere generated. All points were randomly corrupted with\nnoise of \u00b11, before \u03b7\u2208 {10,20, . . . , 50}percent were se-\nlected and corrupted further with noise of \u00b14. For each\n(N, \u03b7)combination, we generated 5 instances, leading to a\ntotal of 50 line fitting problems.\nWe compared NeuroRF-Loihi and RS-CPU; the more\nadvanced methods (Sec. 5.4) were not more energy-efficient\nthan RS-CPU. For both methods, \u03f5inlierwas set to 4and the\nnumber of iterations Kwas set to 100. For NeuroRF-Loihi,\nwe also set M= 200 ,\u03b1= 0.02and\u03b2= 10 .\nWe recorded the distance (22), runtime and dynamic en-\nergy consumption of the methods. Results were averaged\nover 10 trials and the std. dev. was also provided in the\ncase of consensus size. For RS-CPU, runtime was obtained\nwith time python module while the energy consumption was\nmeasured with Intel\u2019s RAPL technology via pyJoules [4].\nFor NeuroRF-Loihi, the energy consumption and runtime\nwere obtained via the built-in Loihi 2 Profiler.\nFig. 4 shows results for N= 20 (plots for N= 10\nare available in Sec. D Supp). Fig. 4a shows the normal-\nized distance in percentage of NeuroRF-Loihi and RS-CPU,\nwhich clearly indicates that both are on par in terms of\nsoluton quality. Fig. 4b shows that NeuroRF-Loihi con-\nsumed about only 15% of the energy required by RS-CPU,\na definitive proof of the much higher energy-efficiency\nof NeuroRF-Loihi. However, Fig. 4c shows that the run-\ntime of NeuroRF-Loihi was greater than RS-CPU. This was\nprobably because NeuroRF-Loihi employed GD for LS,\nwhile RS-CPU solved LS analytically. Also, the speed of\neach Loihi 2 node is not as high as a cutting-edge CPU.5.4. Affine image registration\nWe illustrate the applicability of NeuroRF to robust fitting\non real visual data through an image registration problem.\nAn affine transformation HAwarps a point xin one im-\nage to a corresponding point x\u2032in another image via\nx\u2032=HAex, (23)\nwhere x\u2032= (x\u2032, y\u2032)andex= [xT1] = ( x, y,1)isxin ho-\nmogeneous coordinates. The 2\u00d73affinity matrix HAcan\nbe estimated by solving a linear system constructed from 3\npoint correspondences [36, Chap. 2]. Each correspondence\n\u27e8x\u2032\ni,xi\u27e9contributes two equations, forming a system of six\nequations to estimate 6 parameters. The model dimension\ndof the affine image registration problem is hence 6. When\noutliers are present in the correspondences, robust estima-\ntion methods can be applied with a minimal subset size of\n3. For the conversion to (4) and how to apply our method to\naffine transformation, see Sec. B Supp.\nWe created affine registration instances from 8 scenes\nof VGG Dataset1. In total, 40 image pairs were selected\nand SIFT feature matches were extracted with VLFeat tool-\nbox [65], which were pruned using Lowe\u2019s 2nd nearest\nneighbor test. See Fig. 5 for samples and qualitative results.\nMetric As most VGG ground-truth homographies are\nnear-affine, the estimated affine matrix HAcan be lifted to\na full homography Hest\u2208R3\u00d73by appending a projec-\ntive row [36, Chap. 2]. We follow the evaluation protocol\nin [43, 58, 73] and compute the area under the cumulative\nerror curve (AUC). For each pair, we project the four cor-\nners using ground-truth and estimated homographies, com-\npute the corner error, and report AUC up to a 10-pixel\nthreshold. See, e.g., [58, Sec. 5.2], for details of this metric.\nCompetitors We compared the performance of NeuroRF-\nCPU against RANSAC [29] and the advanced meth-\nods: LO-RANSAC [20], PROSAC [19], Graph-cut\nRANSAC [7] and MAGSAC++ [9]. Except RANSAC,\nwhich was implemented with numpy, other methods were\nbased on OpenCV [14]. Also, LS refinement was executed\non the final consensus for all random sampling methods. We\nset the hyperparameters K= 300 for all 6 solvers, while\nM= 200 and\u03b1= 0.02were selected for NeuroRF-CPU.\nResults We report the AUC at 5 and 10 pixels for these\nmethods on two subsets: the near-affine subset (30 im-\nage pairs) and the entire dataset. As shown in Tab. 1,\nour NeuroRF-CPU achieved competitive AUCs compared\nto RS-CPU and OpenCV\u2019s RANSAC on both subsets. Note\nthat the full dataset includes two non-affine scenes, which\nlikely contribute to higher corner projection errors and\nlower AUC due to their projective transformations. Qualita-\ntive results from our NeuroRF-CPU are provided in Fig. 5.\n1https://www.robots.ox.ac.uk/ vgg/research/affine/\n\n--- Page 8 ---\n2 3 4 5 6 7 8\nmodel dimension020406080100normalized distance (%)\nRS-CPU\nNeuroRS-CPU(a) Effect of dimension d(N= 200 , \u03b7= 0.2)\n0.1 0.2 0.3 0.4 0.5 0.6\nOutlier ratio020406080100normalized distance (%)\nRS-CPU\nNeuroRS-CPU (b) Effect of outlier ratio \u03b7(N= 200 , d= 8)\n100 200 300 400 500\ndata size020406080100normalized distance (%)\nRS-CPU\nNeuroRS-CPU (c) Effect of data size N(\u03b7= 0.2, d= 8)\nFigure 3. Normalized Euclidean distance (%) across various levels of difficulty. Results were averaged over 10 trials for each method.\n0.1 0.2 0.3 0.4 0.5\nOutlier Ratio020406080100normalized distance (%)\nCPU\nLoihi\n(a)\n0.1 0.2 0.3 0.4 0.5\nOutlier Ratio020406080Averaged energy (mJ)8.5x 7.1x 7.2x8.3x9.1x\nCPU\nLoihi (b)\n0.1 0.2 0.3 0.4 0.5\nOutlier Ratio0.000.010.020.030.040.050.060.07Averaged runtime (s)21.6x 21.8x 22.0x 22.0x 21.8x\nCPU\nLoihi (c)\nFigure 4. Performance on neuromorphic hardware. (a) Normalized Euclidean distance (%), (b) average dynamic energy consumption and\n(c) average runtime of NeuroRS-Loihi and RS-CPU on synthetic line fitting instances with N= 20 points, plotted against outlier rate.\n(a) Trees\n(b) Graf\nFigure 5. Green and red lines represent inliers and outliers found\nby NeuroRS-CPU on two affine image registration instances.\n6. Conclusions and future work\nWe designed an SNN for robust fitting and implemented it\non a real neuromorphic processor. Despite the limitations\nof the current neuromorphic hardware, by carefully miti-\ngating the constraints, we were able to establish the viabil-\nity of neuromorphic robust fitting and its superior energyMethodsNear-affine only Full dataset\nAUC@5 AUC@10 AUC@5 AUC@10\nRANSAC [29] 0.396 0.61 0.297 0.458\nMAGSAC++ [9] 0.393 0.599 0.294 0.449\nGC-RANSAC [7] 0.397 0.61 0.298 0.458\nPROSAC [19] 0.396 0.608 0.297 0.456\nLO-RANSAC [20] 0.397 0.611 0.298 0.458\nNeuroRF-CPU 0.396 0.608 0.297 0.456\nTable 1. AUC evaluated at 5 and 10 pixels. Values closer to 1\nindicate better performance.\nefficiency compared to the original CPU version.\n6.1. Future work\nOur current SNN is catered to fitting linear models only.\nExtending to nonlinear models commonly encountered in\ncomputer vision will be useful.\nThe low capacity and precision of Loihi 2 prevent Neu-\nroRF from being competitive against SOTA robust fitting\nmethods. An interesting future work will be to implement\nNeuroRF on a neuromorphic cluster [3] and benchmark\nagainst SOTA methods.\nLast but not least, building more advanced applications\nbased on NeuroRF, such as augmented reality, visual odom-\netry, SLAM and 3D mapping will be of interest.\n\n--- Page 9 ---\nAcknowledgement\nWe acknowledge Intel Labs and the Intel Neuromorphic Re-\nsearch Community (INRC) for granting access to Loihi 2\nand providing technical support. Tat-Jun Chin is SmartSat\nCRC Professorial Chair of Sentient Satellites.\nReferences\n[1] Lava Neuromorphic Computing framework. https://\ngithub.com/lava-nc/lava . 5, 6\n[2] Highlights of Loihi 2 instruction sets. https :\n//download.intel.com/newsroom/2021/new-\ntechnologies / neuromorphic - computing -\nloihi-2-brief.pdf , . 5, 6\n[3] Intel Builds World\u2019s Largest Neuromorphic System to En-\nable More Sustainable AI. https://www.intel.com/\ncontent/www/us/en/newsroom/news/intel-\nbuilds - worlds - largest - neuromorphic -\nsystem.html , . 1, 8\n[4] pyJoules\u2019s documentation. https : / / pyjoules .\nreadthedocs.io/en/latest/ . 7\n[5] Negar Alizadeh and Fernando Castor. Green AI: A prelim-\ninary empirical study on energy consumption in dl models\nacross different runtime infrastructures. In Proceedings of\nthe IEEE/ACM International Conference on AI Engineering-\nSoftware Engineering for AI , 2024. 2\n[6] Md Zahangir Alom, Brian Van Essen, Adam T Moody,\nDavid Peter Widemann, and Tarek M Taha. Quadratic uncon-\nstrained binary optimization (qubo) on neuromorphic com-\nputing system. In 2017 International Joint Conference on\nNeural Networks (IJCNN) , pages 3922\u20133929. IEEE, 2017. 2\n[7] Daniel Barath and Ji \u02c7r\u00b4\u0131 Matas. Graph-cut ransac. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition , pages 6733\u20136741, 2018. 7, 8\n[8] Daniel Barath and Ji \u02c7r\u00b4\u0131 Matas. Graph-cut ransac. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition , 2018. 1\n[9] Daniel Barath, Jana Noskova, Maksym Ivashechkin, and Jiri\nMatas. Magsac++, a fast, reliable and accurate robust estima-\ntor. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 1304\u20131312, 2020. 7, 8\n[10] Jonathan Binas, Giacomo Indiveri, and Michael Pfeiffer.\nSpiking analog vlsi neuron assemblies as constraint satis-\nfaction problem solvers. In 2016 IEEE International Sym-\nposium on Circuits and Systems (ISCAS) , pages 2094\u20132097.\nIEEE, 2016. 2\n[11] Eric Brachmann and Carsten Rother. Neural-Guided\nRANSAC: Learning Where to Sample Model Hypotheses .\nIn2019 IEEE/CVF International Conference on Computer\nVision (ICCV) , pages 4321\u20134330, 2019. 1\n[12] Eric Brachmann and Carsten Rother. Neural-guided ransac:\nLearning where to sample model hypotheses. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, 2019. 2\n[13] Eric Brachmann, Alexander Krull, Sebastian Nowozin,\nJamie Shotton, Frank Michel, Stefan Gumhold, and CarstenRother. DSAC-differentiable ransac for camera localization.\nInProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , 2017. 2\n[14] Gary Bradski, Adrian Kaehler, et al. Opencv. Dr. Dobb\u2019s\njournal of software tools , 3(2), 2000. 7\n[15] Stefano Chiavazza, Svea Marie Meyer, and Yulia San-\ndamirskaya. Low-latency monocular depth estimation using\nevent timing on neuromorphic hardware. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 4071\u20134080, 2023. 2, 5\n[16] Tat-Jun Chin and David Suter. The maximum consensus\nproblem: recent algorithmic advances . Morgan & Claypool\nPublishers, 2017. 3\n[17] Tat-Jun Chin, Zhipeng Cai, and Frank Neumann. Robust\nfitting in computer vision: Easy or hard? In Proceedings of\nthe European Conference on Computer Vision , 2018. 1, 2\n[18] Tat-Jun Chin, David Suter, Shin-Fang Ch\u2019ng, and James\nQuach. Quantum robust fitting. In Proceedings of the Asian\nConference on Computer Vision , 2020. 1\n[19] Ondrej Chum and Jiri Matas. Matching with prosac-\nprogressive sample consensus. In 2005 IEEE computer so-\nciety conference on computer vision and pattern recognition\n(CVPR\u201905) , pages 220\u2013226. IEEE, 2005. 7, 8\n[20] Ond \u02c7rej Chum, Ji \u02c7r\u00b4\u0131 Matas, and Josef Kittler. Locally opti-\nmized ransac. In Joint pattern recognition symposium , pages\n236\u2013243. Springer, 2003. 7, 8\n[21] Ond \u02c7rej Chum, Ji \u02c7r\u00b4\u0131 Matas, and Josef Kittler. Locally op-\ntimized ransac. In Joint Pattern Recognition Symposium ,\n2003. 1\n[22] Ariel Cohen. AI is pushing the world toward an en-\nergy crisis. https://www.forbes.com/sites/\narielcohen / 2024 / 05 / 23 / ai - is - pushing -\nthe-world-towards-an-energy-crisis/ . 1\n[23] Kevin Corder, John V Monaco, and Manuel M Vindiola.\nSolving vertex cover via ising model on a neuromorphic pro-\ncessor. In 2018 IEEE International Symposium on Circuits\nand Systems (ISCAS) , pages 1\u20135. IEEE, 2018. 2\n[24] Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham\nChinya, Yongqiang Cao, Sri Harsha Choday, Georgios Di-\nmou, Prasad Joshi, Nabil Imam, Shweta Jain, et al. Loihi:\nA neuromorphic manycore processor with on-chip learning.\nIeee Micro , 38(1):82\u201399, 2018. 1, 3\n[25] Mike Davies, Andreas Wild, Garrick Orchard, Yulia San-\ndamirskaya, Gabriel A Fonseca Guerra, Prasad Joshi, Philipp\nPlank, and Sumedh R Risbud. Advancing neuromorphic\ncomputing with loihi: A survey of results and outlook. Pro-\nceedings of the IEEE , 109(5):911\u2013934, 2021. 1, 2\n[26] Anh-Dzung Doan, Michele Sasdelli, David Suter, and Tat-\nJun Chin. A hybrid quantum-classical algorithm for robust\nfitting. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , 2022. 2\n[27] Matthew Evanusa, Yulia Sandamirskaya, et al. Event-based\nattention and tracking on neuromorphic hardware. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition Workshops , pages 0\u20130, 2019. 5\n[28] Yan Fang and Ashwin Sanjay Lele. Solving quadratic un-\nconstrained binary optimization with collaborative spiking\n\n--- Page 10 ---\nneural networks. In 2022 IEEE International Conference on\nRebooting Computing (ICRC) , pages 84\u201388. IEEE, 2022. 2\n[29] Martin A Fischler and Robert C Bolles. Random sample\nconsensus: a paradigm for model fitting with applications to\nimage analysis and automated cartography. Communications\nof the ACM , 24(6):381\u2013395, 1981. 1, 6, 7, 8\n[30] Gabriel A Fonseca Guerra and Steve B Furber. Using\nstochastic spiking neural networks on spinnaker to solve con-\nstraint satisfaction problems. Frontiers in neuroscience , 11:\n714, 2017. 2\n[31] Eva Garc \u00b4\u0131a-Mart \u00b4\u0131n, Crefeda Faviola Rodrigues, Graham Ri-\nley, and H \u02daakan Grahn. Estimation of energy consumption in\nmachine learning. Journal of Parallel and Distributed Com-\nputing , 2019. 2\n[32] Stefanos Georgiou, Maria Kechagia, Tushar Sharma, Feder-\nica Sarro, and Ying Zou. Green AI: Do deep learning frame-\nworks have different costs? In Proceedings of the Interna-\ntional Conference on Software Engineering , 2022. 2\n[33] Wulfram Gerstner and Werner M Kistler. Spiking neuron\nmodels: Single neurons, populations, plasticity . Cambridge\nuniversity press, 2002. 3\n[34] Hector A Gonzalez, Jiaxin Huang, Florian Kelber,\nKhaleelulla Khan Nazeer, Tim Langer, Chen Liu, Matthias\nLohrmann, Amirhossein Rostami, Mark Sch \u00a8one, Bernhard\nV ogginger, et al. Spinnaker2: A large-scale neuromorphic\nsystem for event-based and asynchronous machine learning.\narXiv preprint arXiv:2401.04491 , 2024. 1\n[35] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\nDeep Learning . MIT Press, 2016. http://www.\ndeeplearningbook.org . 1\n[36] Richard Hartley and Andrew Zisserman. Multiple view ge-\nometry in computer vision . Cambridge university press,\n2003. 2, 7\n[37] Somayeh Hussaini, Michael Milford, and Tobias Fischer.\nApplications of spiking neural networks in visual place\nrecognition. IEEE Transactions on Robotics , 2025. 2\n[38] Eugene M Izhikevich. Resonate-and-fire neurons. Neural\nnetworks , 14(6-7):883\u2013894, 2001. 3\n[39] Eugene M Izhikevich. Simple model of spiking neurons.\nIEEE Transactions on neural networks , 14(6):1569\u20131572,\n2003. 3\n[40] Zeno Jonke, Stefan Habenschuss, and Wolfgang Maass.\nSolving constraint satisfaction problems with networks of\nspiking neurons. Frontiers in neuroscience , 10:118, 2016.\n2\n[41] Raphaela Kreiser, Alpha Renner, Yulia Sandamirskaya, and\nPanin Pienroj. Pose estimation and map formation with spik-\ning neural networks: towards neuromorphic slam. In 2018\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS) , pages 2159\u20132166, 2018. 2\n[42] Huu Le, Tat-Jun Chin, Anders Eriksson, Thanh-Toan Do,\nand David Suter. Deterministic approximate methods for\nmaximum consensus robust fitting. IEEE Transactions on\nPattern Analysis and Machine Intelligence , 2019. 1\n[43] Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Polle-\nfeys. Lightglue: Local feature matching at light speed. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 17627\u201317638, 2023. 7[44] Ashish Rao Mangalore, Gabriel Andres Fonseca, Sumedh R\nRisbud, Philipp Stratmann, and Andreas Wild. Neuromor-\nphic quadratic programming for efficient and scalable model\npredictive control: Towards advancing speed and energy ef-\nficiency in robotic control. IEEE Robotics & Automation\nMagazine , 2024. 1, 2, 3, 4, 5, 6\n[45] Michael James Martin, Caroline Hughes, Gilberto Moreno,\nEric B Jones, David Sickinger, Sreekant Narumanchi, and\nRay Grout. Energy use in quantum data centers: Scaling\nthe impact of computer architecture, qubit performance, size,\nand thermal parameters. IEEE Transactions on Sustainable\nComputing , 2022. 2\n[46] Jos \u00b4e Mar \u00b4\u0131a Mart \u00b4\u0131nez-Otzeta, Itsaso Rodr \u00b4\u0131guez-Moreno,\nI\u02dcnigo Mendialdua, and Basilio Sierra. Ransac for robotic\napplications: A survey. Sensors , 23(1), 2023. 1\n[47] Matthew S. Mayo and J. Brian Gray. Elemental subsets: The\nbuilding blocks of regression. The American Statistician , 51\n(2):122\u2013129, 1997. 3\n[48] Peter Meer. Robust estimation techniques. Computer Vision:\nA Reference Guide , 2020. 1\n[49] Paul A Merolla, John V Arthur, Rodrigo Alvarez-Icaza, An-\ndrew S Cassidy, Jun Sawada, Filipp Akopyan, Bryan L Jack-\nson, Nabil Imam, Chen Guo, Yutaka Nakamura, et al. A mil-\nlion spiking-neuron integrated circuit with a scalable com-\nmunication network and interface. Science , 345(6197):668\u2013\n673, 2014. 1\n[50] Susan M Mniszewski. Graph partitioning as quadratic un-\nconstrained binary optimization (qubo) on spiking neuro-\nmorphic hardware. In Proceedings of the International Con-\nference on Neuromorphic Systems , pages 1\u20135, 2019. 2\n[51] National Academies of Sciences, Engineering, and Medicine\nand others. Quantum computing: progress and prospects .\n2018. 2\n[52] Tam Nguyen, Anh-Dzung Doan, Tat-Jun Chin, et al. Slack-\nfree spiking neural network formulation for hypergraph min-\nimum vertex cover. Advances in Neural Information Pro-\ncessing Systems , 37:66410\u201366430, 2025. 4\n[53] Garrick Orchard, E Paxon Frady, Daniel Ben Dayan Rubin,\nSophia Sanborn, Sumit Bam Shrestha, Friedrich T Sommer,\nand Mike Davies. Efficient neuromorphic signal processing\nwith loihi 2. In 2021 IEEE Workshop on Signal Processing\nSystems (SiPS) , pages 254\u2013259. IEEE, 2021. 1, 3, 6\n[54] Christoph Ostrau, Christian Klarhorst, Michael Thies, and\nUlrich R \u00a8uckert. Comparing neuromorphic systems by solv-\ning sudoku problems. In 2019 International Conference on\nHigh Performance Computing & Simulation (HPCS) , pages\n521\u2013527. IEEE, 2019. 2\n[55] Trung Thanh Pham, Tat-Jun Chin, Konrad Schindler, and\nDavid Suter. Interacting geometric priors for robust mul-\ntimodel fitting. IEEE Transactions on Image Processing ,\n2014. 1\n[56] Alessandro Pierro, Philipp Stratmann, Gabriel Andres Fon-\nseca Guerra, Sumedh Risbud, Timothy Shea, Ashish Rao\nMangalore, and Andreas Wild. Solving qubo on the loihi 2\nneuromorphic processor. arXiv preprint arXiv:2408.03076 ,\n2024. 2, 4, 5, 6\n[57] Rahul Raguram, Ondrej Chum, Marc Pollefeys, Jiri Matas,\nand Jan-Michael Frahm. USAC: A universal framework for\n\n--- Page 11 ---\nrandom sample consensus. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , 2012. 1\n[58] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,\nand Andrew Rabinovich. Superglue: Learning feature\nmatching with graph neural networks. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition , pages 4938\u20134947, 2020. 7\n[59] Yannick Schnider, Stanis\u0142aw Wo \u00b4zniak, Mathias Gehrig,\nJules Lecomte, Axel V on Arnim, Luca Benini, Davide Scara-\nmuzza, and Angeliki Pantazi. Neuromorphic optical flow\nand real-time implementation with event cameras. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 4129\u20134138, 2023. 2\n[60] Catherine D Schuman, Shruti R Kulkarni, Maryam Parsa,\nJ Parker Mitchell, Prasanna Date, and Bill Kay. Opportu-\nnities for neuromorphic computing algorithms and applica-\ntions. Nature Computational Science , 2(1):10\u201319, 2022. 1,\n2, 3\n[61] Kah Phooi Seng, Paik Jen Lee, and Li Minn Ang. Embedded\nintelligence on fpga: Survey, applications and challenges.\nElectronics , 2021. 2\n[62] Sumit Bam Shrestha, Jonathan Timcheck, Paxon Frady, Leo-\nbardo Campos-Macias, and Mike Davies. Efficient video and\naudio processing with loihi 2. In ICASSP 2024-2024 IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , pages 13481\u201313485. IEEE, 2024. 1, 5\n[63] Bradley H Theilman and James B Aimone. Solving sparse\nfinite element problems on neuromorphic hardware. arXiv\npreprint arXiv:2501.10526 , 2025. 2, 4, 6\n[64] Quoc Huy Tran, Tat-Jun Chin, Wojciech Chojnacki, and\nDavid Suter. Sampling minimal subsets with large spans for\nrobust estimation. International Journal of Computer Vision ,\n2014. 1\n[65] Andrea Vedaldi and Brian Fulkerson. VLFeat: An open and\nportable library of computer vision algorithms, 2012. 7\n[66] Antonio Vitale, Alpha Renner, Celine Nauer, Davide Scara-\nmuzza, and Yulia Sandamirskaya. Event-driven vision and\ncontrol for uavs on a neuromorphic chip. In 2021 IEEE In-\nternational Conference on Robotics and Automation (ICRA) ,\npages 103\u2013109. IEEE, 2021. 2\n[67] John V ourvoulakis, John Lygouras, and John Kalomiros. Ac-\nceleration of ransac algorithm for images with affine trans-\nformation. In IEEE International Conference on Imaging\nSystems and Techniques , 2016. 2\n[68] John V ourvoulakis, John Kalomiros, and John Lygouras.\nFpga accelerator for real-time sift matching with ransac sup-\nport. Microprocessors and Microsystems , 2017.\n[69] John V ourvoulakis, John Kalomiros, and John Lygouras.\nFpga-based architecture of a real-time sift matcher and\nransac algorithm for robotic vision applications. Multime-\ndia Tools and Applications , 2018. 2\n[70] Tong Wei, Yash Patel, Alexander Shekhovtsov, Jiri Matas,\nand Daniel Barath. Generalized differentiable ransac. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision , 2023. 2\n[71] Chris Yakopcic, Nayim Rahman, Tanvir Atahary, Tarek M\nTaha, and Scott Douglass. Leveraging the manycore ar-chitecture of the loihi spiking processor to perform quasi-\ncomplete constraint satisfaction. In 2020 International Joint\nConference on Neural Networks (IJCNN) , pages 1\u20138. IEEE,\n2020. 2\n[72] Frances Fengyi Yang, Michele Sasdelli, and Tat-Jun Chin.\nRobust fitting on a gate quantum computer. In European\nConference on Computer Vision , pages 120\u2013138. Springer,\n2024. 2\n[73] Yuyang Zhang, Jinge Wang, Shibiao Xu, Xiao Liu, and\nXiaopeng Zhang. Mlifeat: Multi-level information fusion\nbased deep local features. In Proceedings of the Asian con-\nference on computer vision , 2020. 7\n[74] Zhengyou Zhang. Parameter estimation techniques: A tuto-\nrial with application to conic fitting. Image and Vision Com-\nputing , 15(1):59\u201376, 1997. 3",
  "project_dir": "artifacts/projects/NeuroRF_Neuromorphic_Robust_Fitting",
  "communication_dir": "artifacts/projects/NeuroRF_Neuromorphic_Robust_Fitting/.agent_comm",
  "assigned_at": "2025-08-16T20:47:23.698342",
  "status": "assigned"
}